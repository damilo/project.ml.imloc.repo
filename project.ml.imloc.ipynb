{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Localization of Objects in Images\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "introduction and problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "familiarize yourself with the data, helps you to better understand and justify your results\n",
    "\n",
    "statistics, visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "+-- dataset\\tiny-imagenet-200\n",
    "    |--- wnids.txt\n",
    "    |--- words.txt\n",
    "    |+-- test\n",
    "         |+-- images (10,000 files)\n",
    "              |--- test_0.JPEG\n",
    "              |--- ...\n",
    "              |--- test_9999.JPEG\n",
    "    |+-- train (200 directories)\n",
    "         |+-- n01443537\n",
    "              |+-- images (500 files)\n",
    "                   |--- n01443537_0.JPEG\n",
    "                   |--- ...\n",
    "                   |--- n01443537_499.JPEG\n",
    "              |--- n01443537_boxes.txt\n",
    "         |+-- ...\n",
    "         |+-- n12267677\n",
    "    |+-- val\n",
    "        |--- val_annotations.txt\n",
    "        |+-- images (10,000 files)\n",
    "             |--- val_0.JPEG\n",
    "             |--- ...\n",
    "             |--- val_9999.JPEG\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DS_PATH = os.path.join ('dataset', 'tiny-imagenet-200')\n",
    "DS_TRAIN_PATH = os.path.join (DS_PATH, 'train')\n",
    "DS_VAL_PATH = os.path.join (DS_PATH, 'val')\n",
    "DS_TEST_PATH = os.path.join (DS_PATH, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image  box_x1  box_y1  box_x2  box_y2\n",
      "0  n01443537_0.JPEG       0      10      63      58\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEKCAYAAADn1WuOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJztvWmwZdlVHvitc+5835RzZmWNElUaLAkJVYsiBISQEF1WyyjsFt3CoBCKotXdwWQDjSQPNDjoQHSEgbbdgV1GMooGg2SGllotDKJAUdimJaXmoVSqQVlVWUMOb37vDmfa/ePe9/a31hvyVqnee/mq1heRkefcve8+++xz3r5rrW8NEkKAw+FwTILkoCfgcDgOD3zDcDgcE8M3DIfDMTF8w3A4HBPDNwyHwzExfMNwOBwTwzcMh8MxMQ5kwxCRO0XkfhF5UETecxBzcDgcTx+y345bIpIC+AaANwK4AOAzAH44hPC1fZ2Iw+F42qgdwDVfA+DBEMLDACAifwDgLQB23DCOd+fCzUdOj8/0BheShI5Ff5HPQzwWs0dK2L7f6Dx2Lunjwlwrb8R55HUtuGUpfY8vtYuAJ1vO4zx2FQvDjidIaK3EXoD7hir2C6XpFs/Fjq/6xbZaql+zRq1uLx6/VtD1aI5pkup+dO2yKPQ8UpoJ3WioKtWPz6tS36fQ92ppvHaS7jyPLT+9W9Z4B/A7Z79DUw5Bz7+gteLf/bRm/qz5YZsHv7i8BABYunwFvdXVq874IDaMswAeo/MLAL5zty/cfOQ0zv3U3aOTRD+WrNXcPM47TdVWNenFDPFWG7lel2ZGL1jR0BcfxpdxhZ7epSn90j9xXWfz+OKZrmp7ZC6Of4W+VqUtfS16cerQL0dd4nlzy4ZHf+BlPLbSY7fb3jxOzTqmId6nlGvxuFhV/RI6r4WhamvRnNMsjn98+qjqd/bYqTiG+QvpL6/ENonrNtXRa1rR9j0/P6/aurN0n/QHng/6qt9wfT1ed21dtTXoB+HYsWObx52ZadUvpw20MFtGqfYtfZ8VPRsJsWMq+uegGsY1zYd6va8sxLXKi9hv9sRxPUY9vnRiNus//uhHAAB3/+L/iklwEDaM7XaxrZuzyLtE5JyInLu8vrQP03I4HFfDQUgYFwDcQOfXA3jCdgoh3A3gbgC4/boXBfRGv4DBbHFlyOOJ2Z25M2/wSaH7BfpVlp7exUGisNTjcQXdb9CP81hd6uk2EsmrTpQq6lNawqhojjVzowntqVadYGFhN4tUyWK3lVJIOpBq++PRIKQKGHWloPN6En/J6kYlUarRLhMuSNVYW1tTbVkR17vZaau24SC2DbP4K1w3amSHpJbU/I4N1+IzXF5ejnNv6F/okm6gMuNLjdSXVLcl6iGS2mTWg9dqZmZGtaW0xuvDbPO4XtdS8loW28pSq29FkY2vO5kt8yAkjM8AuFVEbhGRBoC3AfjoAczD4XA8Tey7hBFCKETkJwH8GYAUwAdCCF/d73k4HI6nj4NQSRBC+DiAjx/EtR0OxzPHgWwYTx8JgCkAW/X3dkW0aqY1rIL0sop09GD0uDyPundaGaqwRRb3Thw/aeSqX1ZEnbe/qsdHK9oqWs3IGIRiSnULSeyXGBajTnpuagwQKevOCdMwhoJmk4Oh6AJdLxGiEWUX+0NlbEFk72Advd7QVGRKtCezOgDQaET9uyC9fEjHANAfRsZjqq51+4SYgDbZEVpNbX9o0RyrXD/PpB2f+9TUFPUz65bubH9gtjQxLy6vo2K0C20XSsLO9h6mexX1m9hrxePSsG/l2IaxZfI7wF3DHQ7HxPANw+FwTIxDopJIdLzaQp2SB6cRb1MSA5Mae2zqfplEcbcxrSmpohNFxCz6ZmGYDlS/PET6LiRavG1IPK/XI5W3lmv6NamxA48R4+m2rZ9kmsTHyOKoVd/ynNSORK8jqwk1RJWqluj1SEhtkkKvY42vR5J1zXgeshOTdWhqkfpWER3bD9rpqiSVanlpRbWpdaRJBfN+MO2ZZfqZsWpQkaqV1vW9KLXDSPVMY4ctKkkchz1mjV/trl6xGdOlWaT5m92O6tdqxmuVuXYHGGajd7AyKupOcAnD4XBMDN8wHA7HxPANw+FwTIxDYcMIIaCoRjpmGoxrLh9bZoipQoomLY0+2Sf6cdjRlOhaLerOy0lsu1LXNN96Eu0RuWmTWtSPaxKDl2pBBzIhRN2zbqJmG2Abg6FVVSQr2W3Mz4EQBWhNQWxLSMA2kZ1p1W1CXjfBrsbNmrHHED9YGhoxJbtFSvec17WO3aJ+DROYttaLAXIrq/HYuj/PHo9BWseO68DFtYUYvzRYj+9A1wSf8bqVJuKV3cbFuMenaZ2OiTI3IQH99fhedbv6PjsUeJmQraZe1+s9JMq4Cvr93rR92BCAHeAShsPhmBi+YTgcjolxKFSSqiZYPTai92oNQ2uR2pEVmjIqU8rxQEltetCU6GIRIyELw43NlzFPwiJ9r2/YrtVavFY/NeQY0apVHiMf6zWdJ0IQ+zWMtM/5MSx9x+esMqS2H6cHseobSaRMsdnkNEqFMPkZiiyuTz3Z3gvRXqwotfrGOTASFcWpJ8xUbctEq3ZIdG82o9getiTJifNqNrRKkszQGhDlmg31uxOIwi0MNVmx6ljX43N0LN9nYVSD+++/f/P4lhtvUm2zs7Obx4p+DZoiLvOoUoVKr3e64W4wYbIflzAcDsfE8A3D4XBMjEOhkgzTCufnRtbiel3rApyebjjU3oAFWYQrutM14zV4uYqegutGZFsM0Uq9XifxvKE9IKtmFG+LVIvxNRLBm6TihFKnv6uXUWxNoZPrpHTfNUNxsHcnO1UawzxYajWaBrKcROuMAvWsSkJqSBgY8XwQ16pBaogE64FL8zcBcnxvu4VDpbW4/ovG07M9RSoKebCuLunMbRWpF0cN+9Ei1a5Gasfamn4/Ekr9GGx6PRLzK8MGFaTmFOQx21/ViYJ6dM6JfACgrOIYge6zPaeDGpnMCoYlybP++HNnSRwOx7MM3zAcDsfE8A3D4XBMjENhw+iFIT43eBgA0DK6PXs9FqWm+VIVtRgV+H6i9dAradQTF4KOIF2hqNSyGcdrtjVV2GiR7m28NGvEe6YUCTrsaZ06Jco4Fe3VV0fUy+uGouMku8w610wVA3IuxFAzbyq5DkdWlpXWeQMl381NUhuQTSBL2RZhvDnZM9X8ZDFdykxqYqJEOdFOozA0ItGnKyvRvrG8qm0dUydj+YC5OZ2Ep0b0Zo/GCCbSWUXeWr6bTw1dOiRb0JASTy8tLqp+N90UqdSmsZtxYuSc1rjR1f3I3INqXT+zhYUrALbS5zvBJQyHwzExfMNwOBwT41CoJIOQ4b7qmwCAU92Tqm1uKtJhdZN/stGOMnl7Oor0/YGucrVyJYqBoaOTjwQKBloZRhGwUWjV5VQzBjK1a1okTIlSqxckpxoVqjaMNGu7o+9lhnKLVpX+npDEn5RR78jWTZ0Mmpat78LpLmshqlvDoQnU43yohQmyozZ2qrR+nkOiYy3dyB6deR7Hs3R6UZEI3ta618papB+PnzixeZyZ594g/a3X03Rmm9RZoSAyq7rUm/G5zK9o2nNINDN7nAL6PlkNu/HGG1W/lDUZUwem2YsPtG+8nBns/buwsKDaumMKWpWX3AUuYTgcjonhG4bD4ZgYvmE4HI6JcShsGGk7xdyLR5GddVOhvSQFfphpV+U+UaRVL+ril9e0rvnk+pV4rbp2ER5S1fRAx7bKNuuaMjRUJNX6DJRo52j3iOq3vBT1y6WBSWw7iDr23JETqm16muZcj/e5OtBUXp4RXWqT3xB9WmR9+tjoxuSOnIipoUG0YiON+rWt0M70Y83wqoHmwXq+bHGHj+dZqWlVtgnw12pN/boP6d6qoO1OWRbvjevOLi9rW8fs0XiBVku/m31KzLvW19/rtKP7do3oUpuEp0H2sGAMT81GtJ8kdG+1lr6XIUVB9zMdFmGTQV8NeyZhiMgHROSSiHyFPjsqIp8QkQfG/x/ZbQyHw3FtYS9Vkt8BcKf57D0A7gkh3ArgnvG5w+E4JNgzlSSEcK+I3Gw+fguA142PPwjgkwDefbWxijLDpZVHAACL64Y6pUQtiSHw6iSPVuR9uWaiLMsqnvcWdZvUo3jHtULqpjpIskZRnNaLsh/px4q+NhhcVP3W5snz04ifrRDHmG4YMb4T58jiMwxl2WxG79HCRInmpGqARfVcrwdTwamJfGQPzjZ5ZtoSf5wwBqlWV1gN4XogMHlBOXHS0CTySUktCxLn2G7rZzboU75PaHViSElnmCbPchMRTRT31MysbiOv3sUVTemyM3CbyjLmA30vFb3DYmhVVr0aRNtWdb2m6/04Z/Z8HQ260XeyDDpiMxk9mxhvGB8LIbxsfL4UQpij9sUQwlXVklPfdjpcfOji1bo5HI5niBNnT+HyhaeuumtcsyyJiLxLRM6JyLn+Su/qX3A4HHuO/WZJLorImRDCkyJyBsClnTqGEO4GcDcAHL356KYY9CPvf5vq12lFMXu6oxmORj16ALLXYN9EXlHx9i3qCo+RkBrSaWrvwiOdTcEJNZOLJC3jxj3ditbxZs1Y1SmdPVvRAeDY8dNxTi19nwmpGtKOnohZZZINUb/MqCQZMUwZMTTFQHsGIotttcqsFYnMs7XoMfuy216q+rXrsW1LGQNa4/4gqmFiPD1zUr0WTVBZu0t5PAPdl/HmXJmPUusLrtMexEU/qhAz3fgs5hf1GDVa0yPHjqk2XuOnLuugsjqpunPT8d2phjZQL6pilQlgq4hRak/Hex4YVfHKanyGf3HvX6q2n3/r/wxg14oRCvstYXwUwDvGx+8A8JF9vr7D4fgWsJe06u8D+BsALxKRCyJyF4D3AXijiDwA4I3jc4fDcUiwlyzJD+/Q9Ia9uqbD4dhbHApPzzpRdKdPaS/HVjPqw21T5p5yq2K4HCm0QV/r3uzV1zSZc2uKpow6tU2aWpI9IrFjkL2j0Y1t7dLQkpRkuF0ziYSraPgdrmvqbbgS6diqHr1Yc0MV1qciIZUbciwvKVFQHq+VlNrg3KRELS1D73bJHpEWUSkeGLtQSolzu23tlYgdapFUxgNySM8iM1GznINZiFYtodct0Lkk5lnUym2Pp2a07erKUlzvsKQXtTsb17ve0LRwj0ogCmULriXaVtNKiS41NgxOblQv4veGZj3Ye3R1VSeejpOYzIhxzbIkDofj2oNvGA6HY2IcCpVESExtmLyGNfbeM2UUyzKKcBWJhKGp90mhZCxKjwFQUmBTSUFlpSn/V6Px6w0ttg5JvWjTkjdLfa31xcubx4NE03BCVFl3+rhq4+ClgqYVzPgtXh7jsMfirqrwbQK7qsBivalZQipJm0TrYc8EPJUUsNXQZQ5ZJVGBWEZkHlL+0GA8IDPyTk2TOP/BQKtXNfaIFKMC1uP6BLrP6RmdQGd+mXKGmqDGhDw4a0YlKdfieq/147xmp7S3KHuEpuad4/XhEou5UXUTSubaG+ycaGcSuIThcDgmhm8YDodjYviG4XA4JsahsGEUpKstLWtaKKGaFKGvdWXyyEbJKruh8jg5S7aq9dyElMiKEsaEVNsAhnVKUmLqU5TknlwN4hgdk7SlyqI7ct2wjXNkqzl6fE611dpR7x1KpJbXTNTsgO/FuIaXZLvJKdy2NNGZIHo3M/VAhKJ552bjnCytWhKny7Q4AKREI7KOLjX928ZJgC1lyRGl0qD6IsY1fK7DNVC03l+vx+uVFKFbN8+2RQmdeit6/NXVaNNoGnf+lJLuFjnZmkxCmxLbJwsefZHsPZxbutBzTCnS1z6LDUwahOoShsPhmBi+YTgcjolxKFQSLkeXmJofLH71hlokzMkbU8hb1O6SFeepbBq6lMT4QGJxzegMDRJNC5OMJCuj511OUaGLRoQNRD+227pUYko5G0tTl2TYi6Jvv4xjrpf6Tvskwg6t5ySXORxG1UiMSiIgb1cTlsu1LZpEAYZcex5m+fb5QwGoUN9AVGHNPLWEnlnDeNYWFI2cUiKiYqCvVaOENxW0SC50LxnNPzFjNIXvU6toK4vRA3dmTntwMo1d0DtclnoerKEEMUmE6Jxp1dJ4IYdkZ6/b7eazG1zCcDgcE8M3DIfDMTEOhUpSI5Hz1IkTppFydfZ03sQ1Eq0zsqrnRnTMiyimJWZJAgdUkeW8Mt50A1IvZrraG/BEN3pmThEzMmeSXV5aiKJvEfQ8Hn38qc3j8nGdrrAk6/kKefJNH9VrNczIW7SrE/QIeYUK5cisGzaF83Zmpnp7IC/Q5Ehsm+3o9WDp95EH71NtL3zhizaP21SlXip9rSn6qesY799mFT1t13rRE/PYtJ5HRuL5fDC5NOfi+hyjILJ0oNXNOqmwK6LVjlM33bJ53DPRfrU0vo8ZeZy2TRIodmLNcqNq0FR6FFDZnNLes9+88M0437otXDmCePCZw+F4tuEbhsPhmBi+YTgcjolxKGwY7IWWmWg7jgJMDJ3JlGiWkfdiZiL2CopCNZ58jWbUS+uUkDU3NFSghLU5DH1Xi/pwtxXHE1vAhDxOl01i2+Wnog1jzZSEbE1FvbekOh8rfT3GyVOn4nim5F+XokZTolyXF7VnbbsR7QpTLa0rr67GMS9fjnaW1hmtNycSX7uWqRXy6KMPbx7PzESP1lZH21xarWinGKzpOVZk7+D6KO22tmGUIA9csQl64vFKL3q3nmzrqhgrdO1VY0ObJcq40dJRqGxnWO/Fi/UMbZvskgQ4IXvSbiUPs4zq4uxAn7qnp8PheNbhG4bD4ZgYh0IlYRr0iSeeUG0c/NNo68Q1nHeTk+mUuamuzol2jMSWEGNXI+9F602XE8WYmWUtuvp6G2DvSgBISJ3IDG07HEZRtTBJberkBTo93aXPtcrA63j0qK6hsb4URes1yvvIQWSAvs/LC/Oq7eh07Lu8HtWTaaMy1IkuTUxA1YXHL2weX1+L6krS0s92iiqUN1Odu3RACXS45GFm6qhwUGMZTB5MorW5Nsh80GpeTol36tN6jn3Kf9pb0QmR0lqci9D8B8Yrtp5QTk/z3jYpYZSiRW1iphUqCak1r4gJCyC6hOFwOCaGbxgOh2Ni+IbhcDgmxqGwYXDU4tLSkmprD6OOdyQ5qtrqHdJzSR8uROuJrMCFSlOz+XB7+0M50J/zt+wuPCS33eUizj9Z0zRcIJq13dH2hxuPXh/7mejMzly0HTS5NkvN1COlJDmXrmj38tluHONkJ9Kvl5/U5W+bFKV73LjpL16+EvvRnC4uXFb9ui2qrWqij4d5pLwHlLim2dF2CrZ9sA0HAMqEbFKUwDdUWoEX6peaPwViY1XimtK486dTcV5zTW0XCpS4pm/oUgr6xZEj0e6UmiTUgc4z4x7foEhcoYhUG/qwsBDtJzYx9NPFXpZKvEFE/kpE7hORr4rIz4w/PyoinxCRB8b/H7naWA6H49rAXqokBYCfCyG8BMAdAH5CRF4K4D0A7gkh3ArgnvG5w+E4BNjL2qpPAnhyfLwqIvcBOAvgLQBeN+72QQCfBPDu3cbiegztRn3Hfjbirk0lClVNB+PVlpHaYT3hKhIJh0TX1YL2XpxqRzF7qqmT34SCcpKuRLF9NtHLnxdRBK+ZMoenqERk0tBt6yTGcw7IUGi16cqVhc3jo0d1bZNV8vxc6MfxThzX/frkzfnooxdU27HZ6Ek5pHyfgyVNZ9aPRzWkv65zqHLGGPZ2FePJyLlkFhY1ZQmhxDukFtgoYhDtOShM3lHyBs6JzqybfLAFXatvkg0NaYykqSnXir2XKcHSWk9T0FOtOOfSUO3snZnQu59nWiWZJ1XRJuiJg23/scW+GD1F5GYArwLwKQCnxpvJxqZycj/m4HA4vnXs+YYhIlMA/gjAPwjBeL3s/r13icg5ETk3WN0+rZjD4dhf7OmGISJ1jDaL3wsh/PH444sicmbcfgbApe2+G0K4O4Rwewjh9pbxoHM4HAeDPbNhyMig8H4A94UQfp2aPgrgHQDeN/7/I1cbi3W1YG0M5N5r9eEmR5q2ot4/Y+pXFi2m3rQdpBhEfTDrRV2zkWpbyrFujKzsmCjOQS/qsn2Ktq2ZJEcZuZsPM30vlJMWtYb+YklJb4WKq9pEv22iJi9ceFS1TZN+P0eU6LyhRNl2c8LQqivkKt6cjtGlDZPlqd2hSFOTlDalRMvLZJtYWl5Q/ZoU9Vs37uU52XQKWu9yqNeDa8hmJoK5LCjZMUcVT2kbxtSRGCncrOt5DMgW1Kzr+itPXY73M2jQ+CaAuUyJZja1Yafona5TLdvC2K4WluI6luX2vuFhQiPGXvphvBbA2wF8WUS+MP7sH2G0UXxYRO4C8CiAH9rDOTgcjmcRe8mS/CcAOyUKfMNeXdfhcOwdDoWnJ6sduUk8y/Uklua12Mri7gyJ2VtqfiRRZE6NF2WNErTWW7FfLZg6GbQ3ivGmq1Fbi+jdynjk1VKmxrSI3O9HcbTd0HNkyrhGXo/Li6ZOSxbnZaNQe2uREsz7cY3nDBXJXqsrK9rr9shc7Fuj52IT7QwpkjU1FHdFdGxJyYgf/vr9qt+AIjAfe+Qx1bZG0bHrVK7QJl9iGjvYso+cnCaNIv6rvuNlqt8tL7wxzjc11GnOSZe1f2JFyXaG/dhvZlp7K/dX4xoP1vTzLKbie9xuU1Sr+ZleofUuqu1VksoT6DgcjmcbvmE4HI6JcShUEvbg7BrxlhPNrJhELWskwg0o5+bsrAniaURRsiZ6D02JFRAqaZflWrxd7RGbYtQaIfUlVWUCTQIdvrSpQaEYIMMKBAqIKrLYtnjpiup3lFiN9ZVl1RYKLgNZp37adYar2VtVA3kUd5vkkZsay/zFxx7fPO52tXpYUP2OhLwSH7pP1y9ZfioGz1nLf0aqjFAtlrYR1TsSGY/cJkQilVCqOMZ/+n/+VPU7f9PZzeOhKWXYnYvqxXd//52qTdajCri8GlmMvlEjZzpRdcyMFyiKGOyW0DsgJkCOGaByBzbEc3o6HI5nHb5hOByOieEbhsPhmBiHwobBkXj1mt7jqop078TUv6DkrSn1g6lzWVFm1LSpPThXFqOuX5EdZLql62TUyW1z0Nc2klY92kimqaZpYehX9i7s2IhX0tODSQYLomN7RDceMbVEhSJvO3Ud8VqwDkvRu4mhj+tkn9E+j0CL2iqyuSzPm+QxZJ/p9zWdydHCOdlEZmqmJijZKRpG/W6yLYGmnxrdfpXsOFOpHv/M6UiXLi1Gb9cjhn5t9+m5r2m70KOPRDvLhx54XLUdvS7aPs7c8ILNYxsFPb8UbRpnzl6v2lq0JvOXY90aMe/w7Gz0Rp03c9xACJMl1nEJw+FwTAzfMBwOx8Q4FCoJezLOdLUqUHAKS5Mfcn4+0lWL85RLE0b8PBO98IY9LT4H8o7k4zzYmiJRfK5Zj3gqw1eSp2oqVoWiOhMmuU6NzlMxj60iSo0Dj6zKQ1SwmGu3SZ2oUTGWpNT3wl6swaheGSUKSqiOShia0pSk5iQ1W/6PjqnJ5EZCne4zNevN1S6FRO12qt+PBiUiss+sSepQYz3OvyV6jMceOL95fOqmW1Rb2Y9jrKzowLGFItbXWVuIakffqJu3vfhvbR4PpvW7X1HCokDuBWsrmn5dIW/RsrL5bEeoWZVvB7iE4XA4JsauEoaIfBeAHwXwPQDOAOgD+AqA/xfA74YQtregOByO5yR2lDBE5E8B/DiAPwNwJ0YbxksB/BMALQAfEZEf3I9JOhyOawO7SRhvDyFcMZ+tAfjc+N8/F5HjW7+2xzAurC2iBxtHTGYuCtu7TAlwi6GmxrjGiNXZm0SJVmXcX7fUdyA7QqNhdGXKfsPUZmooy4rd0G00LM1LtpRKSbbtVzP92MZjEyazC3ydlrgybtdsgykNJVpmsa1NY6Rmqdh1uW5sEwn5xweao50vq9xppd+JOo1BOXrRtUmiyQW+NFHQtV60OSTkxp0PtS3i5S948ebxg4/rpMgz05HObLX1n9qjlJhX6D1ttrW7/ec//f9tHl+6eJNq6/XIjZyilFfNHFcXY2KjDNvTp1X1LbqGb2wWIvKTO9UO2WZDcTgcz2FMYvQ8DeAzIvJhEblT7FbvcDieN7gqrRpC+Cci8k8B/ACAdwL4VyLyYQDvDyE8tNcTHM9h83hgaM9aPYpYXePZePpErGDQbET+tb+mx1imUnJHZnUCE84dWREdO1zTYl9Cc7Qep3Xal0vyckxLo5IUO+/FxZDGN1SkkMpT5/J5qfbm5EjfqtCqRkHqRUG5KKu+qYVBtF9q1DKOLm2RuC+GsuTfHI5+tefMLKepXqsaqR0N87vXonyrBdHf9WDvJaohQxPpPKSLM1V98w03q373P/Tg5vF1Z6/T8zgaBfNPfflLqu1lL3rJ5vE3zn9z83hpVUcHnzh5evP4yUfPq7Yr5N3ZPRKjWl/w0ltVvzte/crN48cv6xKZG7BlJHfCRLRqGP3FPjX+VwA4AuAPReR/n+gqDofjOYGrShgi8tMYZfe+AuC3AfwvIYRcRBIADwD4hb2dosPhuFYwiafncQB/L4TwCH8YQqhE5M17My2NlMTPTkenax9SGYDVZS3OcWKcLlmfVSAagDVSL2wbp7CvSNPIob0XKxJbbXAYC+7sLdpIdYAZynitYET1asjdtEW71ohzblJeybYN1KM0+1VmxHMqhRD62yegAYAaqR01M0d+TsluIi4F+1Um6CmldPnMNnHVeACok/dvy5RR7NJzX82i+pmtrat+68tRFV25ovPBTlPpyzqxS7lhjd76tr+/efw3nz+n2s5TKcnClCj80z+PiXhO3xgD3W697UWq3xMXo9px4sicajt2KpKUl5fi/O/7whdUv5xYqVPH9BgbmO50t/3cYscNQ0SmQghrIYRf3OX7j+3S5nA4nmPYzYbxERH55yLyvSKyuf2IyAtE5C4R2XDocjgczxPsKGGEEN4gIm8C8D8CeO3YF6MAcD9GruHvCCE8tdP3HQ7Hcw+72jBCCB8H8PF9msuOqJEd4dRx7Vw6T1GoT17UZf2Gw+jh1u5Er7t2a1pm67bFAAAgAElEQVT1m5mK56nRhzkSkuuXNGs6SQlr82K85pjCZM/DxCRtCWQ/sRaAkMcrlJkePxBdyJG91sVycX5+pyYwU8vHDbFRszsOD51fNjYmpoRgmjJ1qtegReX/ut1od2q3tBdvg6hZazNq0/uyVEYb19rKouq3QuUXV01bi+Z4lDw2i0TbbT74oX8f5zSjaf2P3fPn1KbfueV+tKeEZrSR1NqaCr+R7Bu2rCTXbWnRtOptvVbt6WifWOprO84G8myygud7Fq0qIi0R+bSIfFFEvioivzz+/BYR+ZSIPCAiHxIRm7jJ4XBco9jL8PYhgNeHEL4dwCsB3CkidwD4NQC/EUK4FcAigLv2cA4Oh+NZxF7WVg0YBasBQH38LwB4PYANLuqDAH4JwG9NOq6tTM1eoDVTr6O3Hj35VpZiJH7e0rJ0txvFxYYYL0qSs9mz0VYMB1eVr6zKQMl1SMQvC6O60PdsmYiSErpUfUPp9impDX0xMTksB+Thar0jExXoRblQzTzYa3W3lCuB1qdu1An2nk3rWrXjyu5TU5HabJnykEzbDte1120gha7Xjx6ctrTjkCjXyniB1ujGO9NRNZovTJAaqRof/YtPqLbZk1F9/ofvfbdq++yXIvV571//583jr9+vS0Ledtttm8f8HgHA9adPbR432lFQX1zVWSf65NG6uqzXYAOJTCY7XLWXiPxfk3y2w3fTceX2SwA+AeAhAEshKt0XAJzd6fsOh+PawiTbyt/iExn9/Lx6ksFDCGUI4ZUArgfwGgAv2a7bdt8VkXeJyDkRObe23Nuui8Ph2GfslkDnvSKyCuAVIrIy/reKkbTwkadzkRDCEoBPArgDwJzIplx+PYAndvjO3SGE20MIt0/Ndrbr4nA49hm7+WH8KoBfFZFfDSG89+kOLCInAOQhhCURaQP4fowMnn8F4K0A/gCjGJWrbj4F0ZKXL2r6K8+odkVXu70207jRLCxE3W3duJBnFAHbOHpStbFHb0puzImlFMllmO0NAEBlSyEJ2wB0Ahp1arIIBMqaU5Va3x6SjpoxPWZsKVPkVp/aOhQ0ZEUn+RZbR7RcJCYql40aFduCbDeyR9Qb2oZRp0QwCdV6qYxtieupJoYKL8mdnZM6D/uaOuRkRg2TFJkjXtvNSHUOhjqqtdaO/Y4c17TqS14RBer77/uyaltZiqlkzj8Y7RZ3fPcdqt+pEzEK9aGHLukxlqMbwbFmrLNaE/3+NSmUoGWNUmPU08lsGJOEt79XRM4CuIn7hxDuvcpXzwD44FiFSQB8OITwMRH5GoA/EJFfAfB5AO+faKYOh+PAMUm06vsAvA3A1xD9iQKAXTeMEMKXALxqm88fxsie4XA4DhkmoVX/LoAXhRCGV+25R+gTjXjpKe2pVlVR/Dx5TNN3t90SI/+W5qKX42OPPKr6tSgS0tKqSlAj6rTMTT5LSoxjPT0D5cvMiL7r2oSWdO3KRnvS+DD0WkJzqbOobhLX1ElUT2s2qU085tKRYjwx6+yJaNSmnFQjdu6UxFLhOTeqtopCgvm4NKqREH3cMJGs5x88H+dEEbqtVL8fSytRNZ2qaQ/L2VZUb2uUJ7VtS2mS92US9Dvx6Dfu2zy+9QU3qLZqLV67W49zvPjIg6rflVtjHs9jczrfZ70Z34n1XvRa7Q10gqiS3rm5WT3/DYhsr6pYTKK4PIyRD4XD4XieY7fw9n+JkerRA/AFEbkHiEkgQgg/vffTczgc1xJ2U0k2soF8FsBH92EuO4MTtZjq1gUl0GEmBAC67WiJzjNKkmOrfbOF2FTnVh6XXCawMklyOKjMjF+RasDjhdKwJAWrHXoQLjsgVm2qOLiNmBzDHtTYaxUGwod0YoKtVPUDm4J0B5ZEzIKwOmSmqM7Za7U0iWs44KzIrThNQXzkTTs05SVqlKyn3TDiPv1pNEhdeekLtWrRno8B20/cqNu+el9UST74r/+NalsfRNX67A2xKvv33KHNewm9jyfPnFFtM3ORQVntxfGyXPstFcO4VoPh9j5NVWVpv+2xG636wYlGcDgczxtMwpJ8GVu9MZcxkkB+JYQwv/VbDofjuYhJWJI/xYhO3Qj8fxtGwugygN8B8Hf2ZGYOh+OawyQbxmtDCK+l8y+LyH8OIbxWRH50rya2E2qWhqOEMbkpd7dKtFlKhou6iZDkOhl5psdgtkm4DoepySGU/GZLHQ62W5CwlplrBdK3rR2EE+OILXdHNG5CbamhPTl61bJooqJVOeLV9GN7iS23SN8L5I1aVfo1Y8q4MhSxjUbeHM/o2OUgrl1S6LYmUZ9sx7EJaEDerpnxnl2hRDONYfxe6OtnVlCZzZfcqkOljsxGavaBhzRdWpJ95qZbbt48Tiu9pqvL0bP0uEkedSWLwv3FS9ELNDeUfEXvXH+HBDqVLf25AyahVadE5Ds3TkTkNQCmxqfbP12Hw/GcxCQSxo8D+ICITGGkiqwA+PFxYuBf3cvJORyOawuTxJJ8BsDLRWQWgIwjTzfw4T2bGYEpH0v/NCkwyNJrOXlApiRMWU/MQZ8840wbqwYJt1U7056JkeNrpKLwtfPc0qrxMN3CWdK1jErCa1JLmLK0AiTrRlYEpe+x6lIaEZw8Tus2+Ex5qsabKTN9n4MkivhWBclq9MxIDduiktCYpnIkOpQXtEn5LXNzzxVRlkum/go/d6HkNGfPnlL9uE5LM9F/TqcokLHT0nU/pijxTrMbgwIvL+r6KGdORiq1Zca4TDlaV5ejqpGaHKrs0frEU09iO1j1eCfs5rj1oyGE3xWRnzWfAwBCCL8+0RUcDsdzBrtJGBvb2fQufRwOx/MIuzlu/Zvx/7+8f9NxOBzXMiZx3LoNoyS9p0IILxORVwD4wRDCr+z57MZgF+HC6P0pRSpa9+E+19Js0a2aBDfZMNow2ibyETsk5rW0ZI0oxlRsZOX2x7mZByrZ7nDLebILrRp2mYe6AWMTYCpVRS5a2wG5swdbw4VtDpRhqDB2kGpIdGZmKOg0PidO6mzXuyLX8GpgXL6nYiKbGtX8aHW0+3efXMXzgZ5jL48R0msU/bk0r20MM514LQl6PZYXYjLewril91OieCl5z5GZI6pfmsT5D9b1GMNeXAMB1X/N9DN76slo63jiCV27ZwOlqf26EyahVf8tgPdinA9qnOfibRON7nA4nlOYZMPohBA+bT5z/wuH43mISfwwrojICzHm5ETkrQC252b2CFy+0CaFYUoxFFrEH5AYWCORrZHq21beo8bjLbC4T8c1I47zrLakTQw8Bovqer4pR6SmxkuT1TKjknAymZRDRrf8HBDtaepw8ByZFi7NmvL3LG3LHrSceGdLpClHodoCLJRsh1US+9z5Wa+v6jybgShSXrfZY0f1fOk9yNd10pl2PaovSSPStF/66tdUv5e+4uWbx0eOaHWCkwMNjScpe5ZW1K8zM6v6peS1utrXkaarvahGLaxE9fvKvA7v+uZjMc/2Tv6caTrJVjDZhvETAO4G8GIReRzANwHsu0u4w+E4eEziuPUwgO8fe3YmIYTVq33H4XA8NzEJS9IE8N8CuBlAjRy3/tmezozAoq/10lRVwo3YWpJ6UZJVPVgPRR7NsAKBxkh2KWUopIeEXQJ5Cgow29Jr57w1KoBoy8Xt+cZwJjhMnZq2ncYI5nP2zLSBgCVrdiRyW1aH2ZStKsn2Xr3WMbWkeeTGwj9PZTFnm1Obx1NHtUrSaEQv0GxNi/s1StQ0oLUPJh3/lcVY9qJv1I6C5pU0dc7QqVZkV0pax9JkkpiZjXO+tKYDx66srG0eP/ZUZD+evHhR9VugEpmt7vY1fooJg88mUUk+glEo+2dBKfocDsfzD5NsGNeHEO7c85k4HI5rHpPQqv9FRF5+9W4Oh+O5jt2CzzZS89UAvFNEHsZIJREAIYTwiv2ZIpR+bcvdMZVnE8awfsyJccToazmVF+y2tDcg06xVYPuDoXc56e0W8wB7TvJ3dqZmrUbJNo2wxY7Dnp48vu4lpH9XwdC2HFFLN2B1ahU5vOU+4wfDLGqvqXHTTIgutcYa7lrytUySnIyo046xD6yQzt6oU02OKW3DEPKwrIyXJkfDrq1H24E0dG2TJxei52cnMxo7jb8+NO9tO9oS2PbzxJUrqh97mX7z8Qv62pdj0pwhvadS1za6RjeGgxnH2k0MvtVoVQBvnmiEq2BcKvEcgMdDCG8WkVswqqt6FMDnALw9hDDZbB0Ox4FiR5UkhPDIbv+exjV+BsB9dP5rAH4jhHArgEUAdz2zqTscjv3GZO5dzxAicj2A/wbA/wbgZ2Uk674ewN8fd/kggF/CKLhtl3HivtY0+Tg58Udmgpw0zUrehdCeh+xRaBOJtGrRQ5R318zkD2XxvN02ag3NI6NygsOB9YCM/QYm9yJXIe92tFh8+kRM1JK24nwzk9uRg8pS40ka6O5y8szMTO0UpqphyyiSCJ6xKtfXonqnE/vVa/p5lqRusQplVZdBFtenb8ZPmKYk9aoyiWWOX39283i4rlWGi0/GeiMlPbOeSfiTk6q0vKzr4jSa8T0ojFq2vhi9MSu6t68/8k3V7/yFxzaPxahejy9E9aVPbgM33HyT6pfTfa+ur2FbTFi9fbJezxy/CeAXEFXyYwCWQtj0L74A4Ox2X3Q4HNce9mzDEJE3A7gUQvgsf7xN1209hkTkXSJyTkTODczu73A4DgZ7qZK8FsAPisibALQAzGAkccyJSG0sZVwP4IntvhxCuBujGBYcv+5YwMr2Jd4cDsf+Yc82jBDCezHKowEReR2Anw8h/IiI/AcAb8WIKXkHRp6kVxts89AmUmmmUQeujLzEiXTTWvxiw+jNjVr84vq6th3k5O7LCWm601OqX6cZqSvrkr1MtSWW1uNxWeh+TdJ5mybZS41sN82GfmzsrjygxLaWPp4+dixeO9d6f0nf468NTaTpcEiJZXo6wjOltSuIZq41zHpXURe3cxzSPOoSbTV1M0aHkuQEQxFzMpyMhOhVW3OGbB8DQ3uuEm27TPaBpKXtR1zf9JHHHlVtgajZxZVl1bZMa3V5mdzLc20jWSEb0nBVr3efwxgoqnXBRLWCqNkZG1E7Rsu6E+yAvbZhbId3Y2QAfRAjm8b7D2AODofjGWBPWZINhBA+CeCT4+OHAbxmt/4Oh+PaxL5sGN8q2LvQlkNk706uhQHYBDWxrTDJQrI61xQxHn+cQIZYxMzQa8M81n5YW9PU1dJibOuRGH/y2EnVL6V5WLUptKjUo6FEKzpnj7+Q67Vao/orAyO28rqmu+UnbUTRtTLqSkn+qTmJ47mJAE7p3tqJvs9WO9be6FIymWbD5OMkj1+rkvRJdOe8qcsmr+agiM9pYNSrBVIZlqjE4uIlHQl6Q3pzvK4Jqe2T2jcwsvwj5KV5mSjWZlerun16/3rmebJSyUmmcqO3nzgV37PbbrtNT+QTXxxdt61VrZ1wECqJw+E4pPANw+FwTIxDoZIkJCI3d8s9aPPiKK9QYjusO4jskjyEvPAK8pzsr62obkuLUbxdXtYW8ZSYnJnpKGa3DdOSkldpsDksOfDNbPMFubIUZFW3ZQgXL8TgpbVlPf8hid1tymE5O2PYoE4Mmqrb3KhUKrDdjCKurRg+IAaiYfK51Mla3+7Ea9sELyvrUaWyKfKHVHaAvUAzU0ogo369Va1GrlBymj6pcg89cl71u9iL/a6/QXtYMrMjbT3H1eHXN48T8gzuG7UjEIMn5s9VyGM5p/d7Zm5O9fuO21+1eXzzC14Ijd/H04FLGA6HY2L4huFwOCaGbxgOh2NiHAobBsPWHmHdvlbXer9KrkP69pakMFRrwyYBHpLnXU5lGnNTsnFIlGJ9SivmMzNRpzx1PFJciTG6cIJjOw81X0N1DtgbsBd19v66pk4XKWHtyqKOrFyjJLhsfzh+9JjqN0f68VRH32edPE7PnozJahrmZ4mjg4Oxg6xS3FAvj3Rj39gfLl+ObYvLpi4JPV62Z6wu6X4LC3ENVq1Nhzxaua5Kb6Dp1+wR8u6sa2rythffunl87vNfUG3rRGuzp64df/pIXO/MpFWqI77fnXa0fx07rr05p6bjcxoO9fgbyIt8288tXMJwOBwTwzcMh8MxMQ6FSsKJcKwHpBLdgynrR+oL18nIq53L/6U1vSQ9Ckrq9aIYmZpkLLMkOjZaOtFJSt6MA0ry0xC7/OxxukPyRegAMwAYkOqxvMgepyYIiVSegfGOFKpanxC1mZigJD5PW1olaVCl9GW6dqup75PpUksfL65EtSEvIj1dmvkO6NleuqzzYHIFeFZJrlzRldfnSa3p9XTwmVYJqTaN8QReJvr1i1/+kmq78YW3bB6fO3dOtbGqUVAu0IZJjjQzE4PsEuMuWpBn6RyVgWy39fv30EMP0Nn2MkKRu0ricDieZfiG4XA4JoZvGA6HY2IcChsGk6Cp0SFrVKczM5TrgNydi4pi+0zC0xpFiTKdBhjallx9myaRyizpkN1uV7UNKUqSXbITMw9RUYZaZ+cxVlc1Pbi8sL3dwlK/052Y5Kc9pV2+m2TDmJ09Qsezqh+7hjcNjdisUdTsYlx7gYkwTiO1Z+ujrFNNkYJsLqWhknv9wbb9AG2v4iQ/NuHPgGu3muTSHI7Aj2Vgkuiy+33R0zajhflIY09Twh/AJIpO4joeO3Fc9Ttx+sTmcWWilIeUEKhOdiK2JQHAlflo4xkMtq92Gmzx2h3gEobD4ZgYvmE4HI6JcShUkj6Jevd/40HV1qI6HGXQ0Zm9jERfiuxrT2mqcGomqhCl8bBskerRaUexsmVybs4dieL+3KwuyccU3fps7HfpyUuqH1OpNkHPSi+qIZdNOb2lpaiS8PTrNU2v5aReNUzJv850vLfuLFHETd2P63DkmRHxaY2PUfKbnonsXSePS1MBUSW8YW1lta+vdelSXINgfve4jOLCSrz2oklsNKAkP2mqvW5TirytiMYuTL5WLp95/NRp1bbWi1G6r75DJ5l7+HysP9LtRjVvZmZa9TtzJlbhmDuq1cMFqoOySEl4Gg393I/Qs12udCT1Bmyu3J3gEobD4ZgYvmE4HI6JcShUklG175F1tzCBYwvrUeS0JRCHRbQic3V1dLRFvN2I4mha1xbm1nQUEY9Qivbpac2EsOoiJkgoI3G6pEyMib4UMrJg27ygC0vR4n5pSXssrq1FxqDVjOJta1qrE925OH8x1coLYmV6fc6JqZmWhJa/adaqQQzK4/NRRJ42eSozqiu5PtTMAnvFcsmB+WW9HqvkwWkD9Vgk71CSop7JBxsS8rBs6Hthj848j+9HwwQWllWchw0Oy0l9qbf0O/eyV3775jEHBdaNB/EFSnpky1ecORlVoKOUmOmpp55S/VLSbo92tVqz09g7wSUMh8MxMXzDcDgcE8M3DIfDMTEOhQ1DiG5sz5gIyTLqq1llSuGVUW8MpHzXTDQfqdS46eabVRvrlDVKyJobz7hqECm0yniccpRrjzwZB5n2uuNaG+uGRlQRtiaSlSNsO1PRtjJ3VCdSqdcjFSxB24LqEteqRsmIxfJt9L1ySxlFmjPTkommLPm8ZxLjrK5GfX5xKdKG6+t6PVjntmX+hOwPHYqMPX1a2ym4nom1YfD4THHbZ5Zl8ZltqWlDj2mqqd+5Jp0fPx69O1Pj/bu2Etcg6+uI2vlLlzeP2dYUTLnF6WZ8J+p1myl7hFqy/edb+k3U6xlCRM4DWAVQAihCCLeLyFEAHwJwM4DzAP67EMLiTmM4HI5rB/uhknxfCOGVIYTbx+fvAXBPCOFWAPeMzx0OxyHAQagkbwHwuvHxBzGqufru3b7AAnjLBH2x6DgstbjYDFT9uxNFzo7x9GwQ5XXpkva+1FtqVENsCUHlEWrmyNJ/QgmAFhc1/dWjfJa21gaXF7TBbaGK3+M8pk0jBnPdEBgqklWUQImIxCbaoWkFU5qyzGIjX9t6el6+FCnXy5cvq7YV8sxcIpWEPUABoE2qlxWn+cl0u3E9Zo9rD9xOO6ordRN8xihJ/Zxf0sIwBytaarKgUokptAcn18aZno2emJYiTunamaGFV6jqO6sktl4Mvy/2vd3ENeLpGQD8uYh8VkTeNf7sVAjhSQAY/39yx287HI5rCnstYbw2hPCEiJwE8AkR+fpVvzHGeIN5FwC0u82r9HY4HPuBPZUwQghPjP+/BOBPALwGwEUROQMA4/8v7fDdu0MIt4cQbm+0dhYXHQ7H/mHPJAwR6QJIQgir4+MfAPDPAHwUwDsAvG/8/0euPlg8bJgkvQVFqBZDrVOz9zNHZ7bb2gZQI9fwyiTQycm9fEgUWlFofbLei5tay0QLchIenuMwtzVWdqYKp7pRB56e1t+br7NeTTdtEsvw0hlGVNV7YftJYvRhZv0SY98o2aZBHS9eeEL1e+ihGKnZN/QxJ5Zps+t5pV3UU1L1q0y35VSHtSDbREN0EpsuJZqx0aolv3REyc91tS1iSEmVamYMtsFYx+uS3N5riO/jINfUaYfWIzPv5mA1UvmcLMrarkDrMUy3dwHnpEO7YS9VklMA/mRsCKoB+PchhP8oIp8B8GERuQvAowB+aA/n4HA4nkXs2YYRQngYwLdv8/k8gDfs1XUdDsfe4VB4enIeT/a2BIBBn0R84wkXiANstqNKUnW1uF9RLkqbwzKnOiK9YRT18tx4lXJkolGbgqqXEud/8vQp1W/Q2zmhC6skoTCenhThmZPKY0XTVj2eF6LF+IIo6YpKL8J4hCa8/qXNpRnv8wJFTD514XHVb3kh0qqWIj55VOe03AAnowH0+qTpzjauGs/fJCWqKCemmPVW9VKoqW2iSes1qtNixuivxzmnZh3ZGzMnVWNgylsyRVqYMoqBKFjOQRrMuzkkL+HtM3oCldVRd4DHkjgcjonhG4bD4ZgYvmE4HI6JcShsGIqUMlQhU0ZWh2R9m7NDNZvahtEhm8ayqTshNGSTkupaV2KOalU1JwCkZCPhpGC1Uo+xNB/dom3tEa79WRdtm+CIzKoR16NuXKbZhmFrlea1+L2KarikJipXyAyQZUZXprof3/jqfZvHwejHU7T+x+e0u/YcuXyzO/iW6FqmS3fJlpVQ5G1q/J9LijwVY3cSooV5CfrGjsDIzJpWNP7KvHYp58DWjGwdeWHq4tD7bbOfgajwJrn9JybCONDfzE6ZtWQL8bs9XMJwOBwTwzcMh8MxMQ6HSsISvYmQZAmr3dKiOkd4CkXpVZnxsKTTTlMn6EnJC5Q9/nLjeVgRdZUbr7mKJpmWcbxmwyR+kSia2oQx/V4UVafaOqnudJfqpZD3YlVo0bTgaEeT5EdI7k5owW10Y0JUXm68NNfIs5EpzI5R0Tiyt2uo34TUhoSig+tGYm6SytAwVCfTyQmJ6lakzynhTy3Rc0xIF81JpVq6oqNrWQvpr2tavyR6emASBbUpiROvsaXCOXq1MnV3WOVMWUW2agepc2lt+z95TwLscDiedfiG4XA4JsahUElYWLKiE5+nhhVgz8OVRc4Pqb0G28tRDbnuphtUW528CBOq8s6qBaBrotRquo2t+HXOddm31ux4LzaQLutHkbZs6O81qZxhg3JzDkqjMqxGBig1ZFNCKlVKVvXU5KmsSCUsh1oE7/fi+DfdeH0cOzEBg6SuWEFY6NocaLiFJaE2G3yWcFAZMQZDw0CwuF/UzHuFuI7lMI7P92jHWJhfUm11UmdXFnUSoYpKZrKnqlW9zMXUacl5anklzd8I/x0klmXcQJgsg45LGA6HY2L4huFwOCaGbxgOh2NiHAobRko2gSlT25I9MWHqhWaku1X9qP8FSzcSzTr/1BXVNjUbKcyZuUhfNmvauzCjqNbKJMbhGqotolLzTOukA6LlUtGP5sbrb4pjmCTDq0vLm8dDol/bJpEP06pMOQPao5BtDEWi7+XR8w9vHh8/eky1TXXivZ08FlO12sS27MVqqc4G0aUpJaexCYeHWbTPNOp6PW684Wycx4lYf/TSlYuq3/nz5zeP11eWVRvbnXgeR6Z1Ap2l5fi9G6+/TrX1KMJ2tqOp8HYnznnQj89sbkaPz+tj68Cw7arPUa7GhjE3EyOwh8Od4lUng0sYDodjYviG4XA4JsahUEk4UUiem+CcsDMF2GTvTlILTIwQGiSe95Y15VoQZZcP4rUbHVPzgyi0etNQkQWJ+xSwlfe0mJ2SSlVPrMoQxdHCeA3mNCaX0yv6Zq0oaUura9Q38tqc6Ua1b2iC8TokSqtEO9BBdmfPnqEWfS3O42lFZD6/TIl2uNwkALS7cR7TMzoJz3lSm+6/PwbBNYwn8HXXRRViZkarDI8//uTm8de/HpPd1xpa/alTgOPM9JRpo5o5Q71WNXo32bvVBvTlpEZal4K0RWPQu7+lumW5vRcvw4PPHA7Hsw7fMBwOx8TwDcPhcEwMCRO6hB4kjp2eCfMXV6/e0eFwPCMcm53C/NLqVQ0ZLmE4HI6JcShYEgB4+8+9EYCu8AToamGNunHqIkt0lkWGoDIZ1WsUyLTa0wFbFQWVCZXbarS1xX2KLPXdWZ1bod2J86qnka45/7XHVL8y48pn2ho/Nze3eWxTDK6vROnryqXL234OAG1yQJqZ0hb9NQrOO3ksps1bvKIrzAstXmmCvjhFYqjYo87k1KBTe5+MK8SSFKZEwLFjRzaPZ2bmVBs/M3YSW1hYUP3YEarV1fPggDlmLkqTS2WaHLmOHT2h2lZW45ouL+lnwWUBmBmy6QbZIcs6ufF7wHO0Dnv87lSmNMTm9e79PCbBodkwqnGWmyS1UYXxFmxOT8q/AqnoLTWemExLNkSPkedE6dIfyHBLApp4br0SS6JBGyl75+nNb6oR/4inzKbQbcRNp9PumLY4ZosiH5fbOnpy2I8vn82zyYG+vX6kUu0fakJ/jMHwd7z+U12EsMwAAAseSURBVPRHPMz1GINBnEdWGE9P+oNpEXW9tqb7La/G6M8l46VZp1q8PKf2lF5Thpj3qqAfGC6LGUy0J29+7H0KaPo4z/Sz5kQ5XJslMa4BHSqZyaUXAaCiubBpwY7Ba1Ca9d54vpOaJvZUJRGRORH5QxH5uojcJyLfJSJHReQTIvLA+P8jVx/J4XBcC9hrG8b/AeA/hhBejFHZxPsAvAfAPSGEWwHcMz53OByHAHtZvX0GwPcC+DEACCFkADIReQuA1427fRDAJwG8+ypjbYqZqUnNz6qGFcXKjPNDRmRGzC7ynUsVNOi8RSqDmIQrlMsERU+Ln2uUaEaq6EnaNgFsHBR3+SmdO/LJCzFwyuZ9ZD26ZpIIMVhXXl7THpx1EsmX5i9tHrca+hUpaYyO8ZysSAf85iPR27Le1PYBtkNZlWeWEss0KZit1dVqGKsJdRNIV5J6WJA6ZEtDsP3EPs8sieMXpGKu9HUinCHlBbVemnyf5tVUJT85v6e1U8xS4Ji1b/A5X8sGV/LfRYAOBNwMQpzM0XNPJYwXALgM4N+JyOdF5LdFpAvgVAjhSQAY/39yt0EcDse1g73cMGoAvgPAb4UQXgVgHU9D/RCRd4nIORE5N+hlV/+Cw+HYc+zlhnEBwIUQwqfG53+I0QZyUUTOAMD4/0vbfTmEcHcI4fYQwu2tTmO7Lg6HY5+xZzaMEMJTIvKYiLwohHA/gDcA+Nr43zsAvG/8/0e+teuQvppp3a2ic0WTGf8B5taDjXglPbpLNUtsRGoJqkGRaxvGkPTLinRvyUyCGyqt1zN2kCHpx9aGwffDem1ZaMmsvxb17WVD0R07EnVl1sVnTMKinGjVelO/Po1mtBHceHNM+NOdnlX9OAnzxYs6qU1JtTdysm+Upg5MSXVUrjuttdqM6sKwbr+8qu/50uPxt8rSu12irmdmYuKkqSkdGdshHxtrj2k2KRlOZWhbvjeiuG2Caqbe7XPnebF9preqI3t3S8Kz8b4kMpnssNd+GD8F4PdEpAHgYQDvxEiq+bCI3AXgUQA/tMdzcDgczxL2dMMIIXwBwO3bNL1hL6/rcDj2BofC0zOEgGxDrDKiHcjVNZRGrKK+TC216tomUgSuKaKptxp5fnICmtzWiMDOnp5cCyLhvKNG/UlS8lrtaHp0djaK9e2WFotZDeHq3wOT4IZVDUtBs7h7+nTMg9ltGyqvHv3s+mva3ZlpykceeyKOvWg8TomKtK7+LOInVAKxZmqbMP19ZVG7fPeowjrXUWkbavbsDbF2ynpf08xLC7Ha+oULj24eX3firOqnPI/1Y1frEcz7MlzZvgp8aZ4Zj7+2pildlTSHXM0Tk9uW26xH5wbVXgU9v53gwWcOh2Ni+IbhcDgmhm8YDodjYhwKGwaE9DWr95MKGcRG6cXbY30Ymp1CSa7h7YZ1Y4665vogUl7DXFNXHBqbGsqV6cd6LV58sGb0Wqqf2jd1S1lnHw5MQlkKz2cbRmYiJCui1CxF1yS7zvFj0V5SmAjMI1Sb5dFVrVOXlMz27Nmo66+saqqQdXE7jzkKred77pmapsoeU9frzevB7wR/DmgKd0tULrmKd4hKvXRZh/sP+3Gt8syG4Me6LTZBtaKup6NtZdrUPbnt2160efzwww+rtpWVuCZc++Xo7FHVr92OrgEcQQsAy+O6KpZu3QkuYTgcjonhG4bD4ZgYhyKnp4hcBvAIgOMArlyl+17jWpgD4POw8HloPN153BRCOHG1Todiw9iAiJwLIWznCPa8moPPw+dxUPNwlcThcEwM3zAcDsfEOGwbxt0HPQFcG3MAfB4WPg+NPZnHobJhOByOg8VhkzAcDscB4lBsGCJyp4jcLyIPisi+ZRkXkQ+IyCUR+Qp9tu9lEkTkBhH5q3Gphq+KyM8cxFxEpCUinxaRL47n8cvjz28RkU+N5/Ghcf6TPYeIpON8sR87qHmIyHkR+bKIfEFEzo0/O4h3ZF9KelzzG4aIpAD+TwB/G8BLAfywiLx0ny7/OwDuNJ8dRJmEAsDPhRBeAuAOAD8xXoP9nssQwOtDCN8O4JUA7hSROwD8GoDfGM9jEcBdezyPDfwMRqUrNnBQ8/i+EMIricY8iHdkf0p6hBCu6X8AvgvAn9H5ewG8dx+vfzOAr9D5/QDOjI/PALj/ANbkIwDeeJBzAdAB8DkA34mRg1Btu+e1h9e/fvxH8HoAH8MoUf5BzOM8gOPms319LgBmAHwTY5vkXs7jmpcwAJwFwEVIL4w/OygcaJkEEbkZwKsAfOog5jJWA76AUfLmTwB4CMBSCJuRXPv1fH4TwC8gRv0dO6B5BAB/LiKfFZF3jT/b7+eybyU9DsOGsV2JlecltSMiUwD+CMA/CCGsXK3/XiCEUIYQXonRL/xrALxku257OQcReTOASyGEz/LH+z2PMV4bQvgOjFTmnxCR792Ha1p8SyU9ng4Ow4ZxAcANdH49gCd26LsfmKhMwrMNEaljtFn8Xgjhjw9yLgAQQljCqGrdHQDmRGQjdnw/ns9rAfygiJwH8AcYqSW/eQDzQAjhifH/lwD8CUab6H4/l2+ppMfTwWHYMD4D4NaxBbwB4G0APnqA8/koRuURgGehTMIkkFFSxvcDuC+E8OsHNRcROSEic+PjNoDvx8i49lcA3rpf8wghvDeEcH0I4WaM3oe/DCH8yH7PQ0S6IjK9cQzgBwB8Bfv8XEIITwF4TEQ2kmdslPR49uex10ahZ8mo8yYA38BIX/7H+3jd3wfwJIAco138Lox05XsAPDD+/+g+zOO7MRKvvwTgC+N/b9rvuQB4BYDPj+fxFQC/OP78BQA+DeBBAP8BQHMfn9HrAHzsIOYxvt4Xx/++uvFuHtA78koA58bP5v8GcGQv5uGeng6HY2IcBpXE4XBcI/ANw+FwTAzfMBwOx8TwDcPhcEwM3zAcDsfE8A3D4XBMDN8wHLtCRD6+4axlPv8lEfn58fGPich11HZeRI5PMParROS3r9LnJ0Xknc9k7o5nH75hOHZFCOFNYeQGvht+DMB1V+mzHf4RgH95lT4fAPDTz2Bsxx7AN4znMUTkF0Tkp8fHvyEifzk+foOI/O74eFNaEJF/PE5k9BcAXjT+7K0Abgfwe+MkMht1+X5KRD43Ti7z4m2uPQ3gFSGEL47P/4WI/OL4+L8WkXtFJAkh9ACcF5HX7OVaOCaDbxjPb9wL4HvGx7cDmBoHuX03gL/mjiLyaoziNl4F4O8B+K8AIITwhxi5JP9IGCWR2SjeeSWMojh/C8DPb3Pt2zFyL9/AewD89yLyfQD+BYB3hhA2QtfP0TwdBwjfMJ7f+CyAV49/7YcA/gajP+Tvgdkwxp/9SQihF0ah9VcLANyIqP0sRkmILM5glMMBADCWJP4HjHJs/KsQwkPU9xKemcrjeJZxOKq3O/YEIYR8HCL+TgD/BaPApe8D8ELo1HebX3kaw2+UkS+x/XvWB9Ayn70cwDy2bg6tcX/HAcMlDMe9GKkM92IkVfxPAL4QtkYl3gvg74pIeyyR/B1qWwUw/TSvex+Ab9s4EZGbAPwcRirP3xaR76S+t0GrL44Dgm8Yjr/GSD34mxDCRQADbFVHEEL4HIAPYRRa/0emz+8A+NfG6LkrQghfBzArItOU7+PnwyghzV0AfltENiSQ1wL4i2dyc45nFx7e7jgwiMg/BLAaQtjRF0NEXgXgZ0MIb9+/mTl2gksYjoPEbyHaOnbCcQD/dB/m4pgALmE4HI6J4RKGw+GYGL5hOByOieEbhsPhmBi+YTgcjonhG4bD4ZgY/z8FLj836rLlAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check out the properties of the images (only one picture)\n",
    "# display an image\n",
    "# annotations of train images\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "DS_TRAIN_IMAGE = os.path.join (DS_TRAIN_PATH, 'n01443537', 'images', 'n01443537_0.JPEG')\n",
    "train_img = io.imread (DS_TRAIN_IMAGE)\n",
    "\n",
    "DS_TRAIN_IMAGE_ANNOT = os.path.join (DS_TRAIN_PATH, 'n01443537', 'n01443537_boxes.txt')\n",
    "img_annot_df = pd.read_csv (DS_TRAIN_IMAGE_ANNOT, sep='\\t', header=None, names=['image', 'box_x1', 'box_y1', 'box_x2', 'box_y2'])\n",
    "print (img_annot_df.head (1))\n",
    "\n",
    "fig, axs = plt.subplots (1, 1)\n",
    "axs.set_ylabel ('height (y)')\n",
    "axs.set_xlabel ('width (x)')\n",
    "axs.imshow (train_img)\n",
    "rect = patches.Rectangle ((img_annot_df.iloc[0][1],img_annot_df.iloc[0][2]),\n",
    "                          (img_annot_df.iloc[0][3] - img_annot_df.iloc[0][1]),\n",
    "                          (img_annot_df.iloc[0][4] - img_annot_df.iloc[0][2]),\n",
    "                          linewidth=2,edgecolor='r',facecolor='none')\n",
    "axs.add_patch (rect)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wnid file (wnids.txt)\n",
    "holds all WordNet IDs of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 1 columns):\n",
      "wnid    200 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.6+ KB\n",
      "None\n",
      "             wnid\n",
      "count         200\n",
      "unique        200\n",
      "top     n02403003\n",
      "freq            1\n",
      "         wnid\n",
      "57  n02481823\n",
      "26  n03085013\n",
      "45  n03983396\n"
     ]
    }
   ],
   "source": [
    "# check out the wnid annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WNID_FILE = os.path.join (DS_PATH, 'wnids.txt')\n",
    "wnid_df = pd.read_csv (WNID_FILE, header=None, names=['wnid'])\n",
    "\n",
    "print (wnid_df.info ())\n",
    "print (wnid_df.describe ())\n",
    "print (wnid_df.sample (3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words file (words.txt)\n",
    "holds all WordNet IDs and their description(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82115 entries, 0 to 82114\n",
      "Data columns (total 2 columns):\n",
      "wnid    82115 non-null object\n",
      "desc    82114 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "             wnid   desc\n",
      "count       82115  82114\n",
      "unique      82115  76002\n",
      "top     n12758399   head\n",
      "freq            1     16\n",
      "            wnid                       desc\n",
      "8848   n01728266                 hoop snake\n",
      "1887   n00386164               parcellation\n",
      "77670  n14503060  atypicality, untypicality\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WNID_WORDS_FILE = os.path.join (DS_PATH, 'words.txt')\n",
    "wnid_words_df = pd.read_csv (WNID_WORDS_FILE, sep='\\t', header=None, names=['wnid', 'desc'])\n",
    "\n",
    "print (wnid_words_df.info ())\n",
    "print (wnid_words_df.describe ())\n",
    "print (wnid_words_df.sample (3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n01443537 goldfish, Carassius auratus\n"
     ]
    }
   ],
   "source": [
    "# get the description of a wnid\n",
    "#wnid = wnid_df['wnid'].sample ().values[0]\n",
    "wnid = 'n01443537'\n",
    "wnid_desc = wnid_words_df[wnid_words_df['wnid'] == wnid]['desc'].values[0]\n",
    "\n",
    "print (wnid, wnid_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "some other algorithm / software / result to compare this algorithm against (by using the defined metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random guess as benchmark for both classification and localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split project here into\n",
    "# - multiclass classification\n",
    "# - regression for localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "used metric for classification is categorial cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train data__\n",
    "<pre>\n",
    "+-- dataset\\tiny-imagenet-200\n",
    "    |+-- train (200 directories)\n",
    "         |+-- n01443537\n",
    "              |+-- images (500 files)\n",
    "                   |--- n01443537_0.JPEG\n",
    "                   |--- ...\n",
    "                   |--- n01443537_499.JPEG\n",
    "              |--- n01443537_boxes.txt\n",
    "         |+-- ...\n",
    "         |+-- n12267677\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 200 keys (classes) in the dictionary, each holding 500 values (image paths).\n",
      "\n",
      "Example: key (class) = n02236044 , values (image paths) = ['dataset\\\\tiny-imagenet-200\\\\train\\\\n02236044\\\\images\\\\n02236044_0.JPEG', 'dataset\\\\tiny-imagenet-200\\\\train\\\\n02236044\\\\images\\\\n02236044_1.JPEG'] ... ['dataset\\\\tiny-imagenet-200\\\\train\\\\n02236044\\\\images\\\\n02236044_98.JPEG', 'dataset\\\\tiny-imagenet-200\\\\train\\\\n02236044\\\\images\\\\n02236044_99.JPEG']\n"
     ]
    }
   ],
   "source": [
    "# read in all images from training dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "# get a list of all image class directories\n",
    "train_data = defaultdict (list)\n",
    "train_path_entries = os.listdir (DS_TRAIN_PATH)\n",
    "for i in range (len (train_path_entries)):\n",
    "    entry_i_imgs_p = DS_TRAIN_PATH + os.sep + train_path_entries[i] + os.sep + 'images'\n",
    "    \n",
    "    # add data as key-value pair for later use\n",
    "    k = train_path_entries[i]\n",
    "    v = [entry_i_imgs_p + os.sep + img for img in os.listdir (entry_i_imgs_p)]\n",
    "    train_data[k] = v\n",
    "\n",
    "print ('Now there are {} keys (classes) in the dictionary, each holding {} values (image paths).'\\\n",
    "       .format (len (train_data.keys ()), len (train_data[list (train_data.keys ())[0]])))\n",
    "print ()\n",
    "print ('Example:',\\\n",
    "       'key (class) =', list (train_data.keys ())[42],\\\n",
    "       ', values (image paths) =',\\\n",
    "       train_data[list (train_data.keys())[42]][:2], '...',\\\n",
    "       train_data[list (train_data.keys())[42]][-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN expects tensors in form (#, w, h, d) > read in images and create tensor per class.\n",
    "\n",
    "[!] mixed BW and RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "def image_path_to_tensor (img_path_list):\n",
    "    img_list = []\n",
    "    for img_path in img_path_list:\n",
    "        img_arr = imread (img_path)\n",
    "        # check depth for BW image\n",
    "        if (len (img_arr.shape) < 3):\n",
    "            img_arr = gray2rgb (img_arr)\n",
    "        \n",
    "        img_arr = np.expand_dims (img_arr, axis=0)\n",
    "        img_list.append (img_arr)\n",
    "    \n",
    "    return np.vstack (img_list)\n",
    "    \n",
    "# unit test\n",
    "# (image_path_to_tensor (train_data[list (train_data.keys())[42]][:5])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images of wnid class n12267677, ( 200 /  200)\n",
      "\n",
      "Shape of train data tensor: (100000, 64, 64, 3)\n",
      "Shape of train data target vector: (100000,)\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor_list = []\n",
    "train_data_targets_list = []\n",
    "i = 1\n",
    "for img_class, img_paths in train_data.items ():\n",
    "    #pass\n",
    "    # read images\n",
    "    print ('\\rReading images of wnid class {}, ({:4} / {:4})'.format (img_class, i, len (train_data.keys ())), end='', flush=True)\n",
    "    img_class_tensor = image_path_to_tensor (train_data[img_class])\n",
    "    # add images to train data tensor\n",
    "    train_data_tensor_list.append (img_class_tensor)\n",
    "    # write image class to target variable\n",
    "    train_data_targets_list.append ([img_class for x in range (len (train_data[img_class]))])\n",
    "    # blub\n",
    "    i += 1\n",
    "\n",
    "train_data_tensor = (np.vstack (train_data_tensor_list)).astype (dtype='f4', copy=False) / 255 # incl. normalization [0, 255] -> [0, 1]\n",
    "train_data_targets = np.hstack (train_data_targets_list)\n",
    "\n",
    "# clean up\n",
    "del train_data_tensor_list\n",
    "del train_data_targets_list\n",
    "del train_data\n",
    "\n",
    "print ('\\n')\n",
    "print ('Shape of train data tensor:', train_data_tensor.shape)\n",
    "print ('Shape of train data target vector:', train_data_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__validation data__\n",
    "<pre>\n",
    "+-- dataset\\tiny-imagenet-200\n",
    "    |+-- val\n",
    "        |--- val_annotations.txt\n",
    "        |+-- images (10,000 files)\n",
    "             |--- val_0.JPEG\n",
    "             |--- ...\n",
    "             |--- val_9999.JPEG\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 10000 values (image paths) in validation set.\n",
      "\n",
      "Example: key = val_imgs , values (image paths) = ['dataset\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_0.JPEG', 'dataset\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_1.JPEG'] ... ['dataset\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_9998.JPEG', 'dataset\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_9999.JPEG']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# get a list of all image class directories\n",
    "val_data = defaultdict (list)\n",
    "entry_i_imgs_p = DS_VAL_PATH + os.sep + 'images'\n",
    "k = 'val_imgs'\n",
    "v = [entry_i_imgs_p + os.sep + img for img in os.listdir (entry_i_imgs_p)]\n",
    "val_data[k] = v\n",
    "\n",
    "# image annotations (contain wnid class)\n",
    "DS_VAL_PATH_ANNOTS = DS_VAL_PATH + os.sep + 'val_annotations.txt'\n",
    "val_annots_df = pd.read_csv (DS_VAL_PATH_ANNOTS, sep='\\t', header=None, names=['image', 'wnid', 'box_x1', 'box_y1', 'box_x2', 'box_y2'])\n",
    "\n",
    "\n",
    "print ('Now there are {} values (image paths) in validation set.'\\\n",
    "       .format (len (val_data['val_imgs'])))\n",
    "print ()\n",
    "print ('Example:',\\\n",
    "       'key =', list (val_data.keys ())[0],\\\n",
    "       ', values (image paths) =',\\\n",
    "       val_data[list (val_data.keys())[0]][:2], '...',\\\n",
    "       val_data[list (val_data.keys())[0]][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading validation images, (10000 / 10000)\n",
      "\n",
      "Shape of val data tensor: (10000, 64, 64, 3)\n",
      "Shape of val data target vector: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# read in all images from validation dataset\n",
    "val_data_tensor_list = []\n",
    "val_data_targets_list = []\n",
    "i = 1\n",
    "for img_path in val_data['val_imgs']:\n",
    "    # read images\n",
    "    print ('\\rReading validation images, ({:5} / {:5})'.format (i, len (val_data['val_imgs'])), end='', flush=True)\n",
    "    img_class_tensor = image_path_to_tensor ([img_path])\n",
    "    # add images to  tensor\n",
    "    val_data_tensor_list.append (img_class_tensor)\n",
    "    # write image class to target variable\n",
    "    img_name = os.path.basename (img_path)\n",
    "    img_wnid = val_annots_df[val_annots_df['image'] == img_name]['wnid'].values[0]\n",
    "    val_data_targets_list.append (img_wnid)\n",
    "    # blub\n",
    "    i += 1\n",
    "\n",
    "val_data_tensor = (np.vstack (val_data_tensor_list)).astype (dtype='f4', copy=False) / 255 # incl. normalization [0, 255] -> [0, 1]\n",
    "val_data_targets = np.hstack (val_data_targets_list)\n",
    "\n",
    "# clean up\n",
    "del val_data_tensor_list\n",
    "del val_data_targets_list\n",
    "del val_data\n",
    "\n",
    "print ('\\n')\n",
    "print ('Shape of val data tensor:', val_data_tensor.shape)\n",
    "print ('Shape of val data target vector:', val_data_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode targets\n",
    "import pandas as pd\n",
    "\n",
    "train_data_targets_onehot = pd.get_dummies (train_data_targets)\n",
    "\n",
    "val_data_targets_onehot = pd.DataFrame (\n",
    "    data= np.zeros ((val_data_targets.shape[0], train_data_targets_onehot.shape[1]),  dtype=train_data_targets_onehot.values.dtype),\n",
    "    columns=train_data_targets_onehot.columns.values)\n",
    "\n",
    "for i in range (val_data_targets.shape[0]):\n",
    "    cur_wnid = val_data_targets[i]\n",
    "    val_data_targets_onehot.iloc[i][cur_wnid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arrays of everything (i.e. get rid of index and column)\n",
    "train_targets = train_data_targets_onehot.values\n",
    "val_targets = val_data_targets_onehot.values\n",
    "\n",
    "# save the columns once to identify the wnid later\n",
    "targets_names = train_data_targets_onehot.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "   type : <class 'numpy.ndarray'>\n",
      "   dtype: float32\n",
      "   shape: (100000, 64, 64, 3)\n",
      "\n",
      "validation data:\n",
      "   type : <class 'numpy.ndarray'>\n",
      "   dtype: float32\n",
      "   shape: (10000, 64, 64, 3)\n",
      "\n",
      "target data:\n",
      "   type : <class 'numpy.ndarray'>\n",
      "   dtype: uint8\n",
      "   shape: (100000, 200)\n",
      "   names: ['n01443537' 'n01629819' 'n01641577'] ... ['n09332890' 'n09428293' 'n12267677']\n"
     ]
    }
   ],
   "source": [
    "print ('train data:')\n",
    "print ('  ', 'type :', type (train_data_tensor))\n",
    "print ('  ', 'dtype:', train_data_tensor.dtype)\n",
    "print ('  ', 'shape:', train_data_tensor.shape)\n",
    "\n",
    "print ()\n",
    "\n",
    "print ('validation data:')\n",
    "print ('  ', 'type :', type (val_data_tensor))\n",
    "print ('  ', 'dtype:', val_data_tensor.dtype)\n",
    "print ('  ', 'shape:', val_data_tensor.shape)\n",
    "\n",
    "print ()\n",
    "\n",
    "print ('target data:')\n",
    "print ('  ', 'type :', type (train_targets))\n",
    "print ('  ', 'dtype:', train_targets.dtype)\n",
    "print ('  ', 'shape:', train_targets.shape)\n",
    "print ('  ', 'names:', targets_names[:3], '...', targets_names[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "RND_STATE = 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_shuffled, _, train_targets_shuffled, _ = train_test_split (train_data_tensor, train_targets,\n",
    "                                                                test_size=0.0, random_state=RND_STATE, shuffle=True)\n",
    "\n",
    "val_data_shuffled, _, val_targets_shuffled, _ = train_test_split (val_data_tensor, val_targets,\n",
    "                                                                  test_size=0.0, random_state=RND_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "del train_data_tensor\n",
    "del train_targets\n",
    "del val_data_tensor\n",
    "del val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "\n",
    "# basis: VGG16 model (https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md)\n",
    "\n",
    "# input layer\n",
    "input_shape = train_data_shuffled.shape[1:]\n",
    "inputs = layers.Input (shape=input_shape, name='input')\n",
    "\n",
    "# hidden layer\n",
    "net = layers.Dense (\n",
    "    units = 512,\n",
    "    activation = 'relu',\n",
    "    name = 'fc1'\n",
    ") (inputs)\n",
    "\n",
    "net = layers.Dense (\n",
    "    units = 256,\n",
    "    activation = 'relu',\n",
    "    name = 'fc2'\n",
    ") (net)\n",
    "\n",
    "net = layers.Dense (\n",
    "    units = 128,\n",
    "    activation = 'relu',\n",
    "    name = 'fc3'\n",
    ") (net)\n",
    "\n",
    "# output layer\n",
    "outputs = layers.Dense (\n",
    "    units=1,\n",
    "    activation='linear',\n",
    "    name='output'\n",
    ") (net)\n",
    "\n",
    "# create optimizer\n",
    "opt_sgd = optimizers.SGD (lr=0.001)\n",
    "\n",
    "# build and compile model\n",
    "mlp = models.Model (inputs=inputs, outputs=outputs)\n",
    "mlp.compile (optimizer=opt_sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "pool4 (GlobalAveragePooling2 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc5 (Dense)                  (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 200)               102600    \n",
      "=================================================================\n",
      "Total params: 2,178,888\n",
      "Trainable params: 2,178,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "100000/100000 [==============================] - 1892s 19ms/step - loss: 5.2981 - acc: 0.0050 - val_loss: 5.2976 - val_acc: 0.0047\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.29759, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 2/20\n",
      "100000/100000 [==============================] - 1886s 19ms/step - loss: 5.2974 - acc: 0.0050 - val_loss: 5.2968 - val_acc: 0.0041\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.29759 to 5.29680, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 3/20\n",
      "100000/100000 [==============================] - 1888s 19ms/step - loss: 5.2964 - acc: 0.0057 - val_loss: 5.2955 - val_acc: 0.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.29680 to 5.29551, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 4/20\n",
      "100000/100000 [==============================] - 1889s 19ms/step - loss: 5.2945 - acc: 0.0092 - val_loss: 5.2926 - val_acc: 0.0100\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.29551 to 5.29264, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 5/20\n",
      "100000/100000 [==============================] - 1890s 19ms/step - loss: 5.2888 - acc: 0.0106 - val_loss: 5.2811 - val_acc: 0.0096\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.29264 to 5.28107, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 6/20\n",
      "100000/100000 [==============================] - 1890s 19ms/step - loss: 5.2444 - acc: 0.0108 - val_loss: 5.1696 - val_acc: 0.0132\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.28107 to 5.16962, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 7/20\n",
      "100000/100000 [==============================] - 1890s 19ms/step - loss: 5.1284 - acc: 0.0146 - val_loss: 5.0950 - val_acc: 0.0157\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.16962 to 5.09497, saving model to project.ml.imloc.weights.best.hdf5\n",
      "Epoch 8/20\n",
      " 95900/100000 [===========================>..] - ETA: 1:15 - loss: 5.0971 - acc: 0.0160"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-600ede6fb6a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m history = cnn_model.fit (train_data_shuffled, train_targets_shuffled,\n\u001b[0;32m     12\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data_shuffled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_targets_shuffled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m           epochs=epochs, batch_size=100, callbacks=[checkpointer], verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\_pyenvs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\_pyenvs\\ml\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_pyenvs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_pyenvs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_pyenvs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "checkpointer = ModelCheckpoint (\n",
    "                    filepath='project.ml.imloc.weights.best.hdf5', \n",
    "                    verbose=1,\n",
    "                    save_best_only=True)\n",
    "\n",
    "history = cnn_model.fit (train_data_shuffled, train_targets_shuffled,\n",
    "          validation_data=(val_data_shuffled, val_targets_shuffled),\n",
    "          epochs=epochs, batch_size=100, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "define the metric to measure the quality of the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "split data, define and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Validation\n",
    "results and justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dataset directory with number of folders and files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = os.path.join ('dataset', 'tiny-imagenet-200')\n",
    "\n",
    "print ('+--', ds_path)\n",
    "path_entries = os.listdir (ds_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_path = os.path.join (ds_path, 'test')\n",
    "\n",
    "print ('+--', ds_test_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_test_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_test_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_images_path = os.path.join (ds_test_path, 'images')\n",
    "\n",
    "print ('+--', ds_test_images_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_test_images_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_test_images_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val_path = os.path.join (ds_path, 'val')\n",
    "\n",
    "print ('+--', ds_val_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_val_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_val_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val_images_path = os.path.join (ds_val_path, 'images')\n",
    "\n",
    "print ('+--', ds_val_images_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_val_images_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_val_images_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_path = os.path.join (ds_path, 'train')\n",
    "\n",
    "print ('+--', ds_train_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_train_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_train_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_wnid_path = os.path.join (ds_train_path, 'n01443537')\n",
    "\n",
    "print ('+--', ds_train_wnid_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_train_wnid_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_train_wnid_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_wnid_images_path = os.path.join (ds_train_wnid_path, 'images')\n",
    "\n",
    "print ('+--', ds_train_wnid_images_path)\n",
    "num_dirs = 0\n",
    "num_files = 0\n",
    "path_entries = os.listdir (ds_train_wnid_images_path)\n",
    "for entry in path_entries:\n",
    "    entry_path = os.path.join (ds_train_wnid_images_path, entry)\n",
    "    if (os.path.isdir (entry_path)):\n",
    "        print ('    |+--', entry)\n",
    "        num_dirs+=1\n",
    "    if (os.path.isfile (entry_path)):\n",
    "        print ('    |---', entry)\n",
    "        num_files+=1\n",
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('directories:', num_dirs, ', files:', num_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
